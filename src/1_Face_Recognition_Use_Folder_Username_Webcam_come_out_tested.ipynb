{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "from PIL import Image\n",
    "import glob\n",
    "import re\n",
    "import ntpath\n",
    "import os\n",
    "import dlib\n",
    "import time\n",
    "from imutils.face_utils import FaceAligner\n",
    "import sqlite3\n",
    "from imutils.face_utils import rect_to_bb\n",
    "import imutils\n",
    "from imutils import face_utils\n",
    "import os\n",
    "import shutil\n",
    "import inception_resnet_v1\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"../data/unknown_people/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"../data/unknown_people/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = sqlite3.connect('../data/people.db')\n",
    "c = people.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "no such table: people_known",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-449de1bb8ab7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# show name people know name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'select * from people_known order by old'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: people_known"
     ]
    }
   ],
   "source": [
    "# show name people know name\n",
    "for row in c.execute('select * from people_known order by old'):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Dam Van Tai', 1, 24)\n"
     ]
    }
   ],
   "source": [
    "# show name people in room equal empty\n",
    "# c.execute('delete from people_inroom')\n",
    "# people.commit()\n",
    "for row in c.execute('select * from people_inroom order by old'):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('unknown_0', 1, 33)\n"
     ]
    }
   ],
   "source": [
    "# show id people unknown name\n",
    "# c.execute('delete from people_unknown')\n",
    "# people.commit()\n",
    "for row in c.execute('select * from people_unknown order by old'):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recognition face center\n",
    "def face_center(image):\n",
    "#     detector = dlib.get_frontal_face_detector()\n",
    "#     predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "    image_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    rects = detector(image_gray, 1)\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        points = predictor(image_gray, rect)\n",
    "        points = face_utils.shape_to_np(points)\n",
    "        center_face = points[37][1] - points[46][1]\n",
    "#         cross_face1 = points[49][0] - points[58][0]\n",
    "#         cross_face2 = points[58][0] - points[55][0]\n",
    "#         print(cross_face1)\n",
    "#         print(cross_face2)\n",
    "#         cross = cross_face1 - cross_face2\n",
    "#         print(cross)\n",
    "#         if center_face < 10 & center_face > -10 & cross < 20 & cross > -20:\n",
    "        if center_face < 10 & center_face > -10:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _css_to_rect(css):\n",
    "    \"\"\"\n",
    "    Convert a tuple in (top, right, bottom, left) order to a dlib `rect` object\n",
    "    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n",
    "    :return: a dlib `rect` object\n",
    "    \"\"\"\n",
    "    return dlib.rectangle(css[3], css[0], css[1], css[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../gender_age_tf/models/model.ckpt-14001\n",
      "restore model!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() \n",
    "# sess = tf.InteractiveSession()\n",
    "sess = tf.Session()\n",
    "images_pl = tf.placeholder(tf.float32, shape=[None, 160, 160, 3], name='input_image')\n",
    "images_norm = tf.map_fn(lambda frame: tf.image.per_image_standardization(frame), images_pl)\n",
    "train_mode = tf.placeholder(tf.bool)\n",
    "age_logits, gender_logits, _ = inception_resnet_v1.inference(images_norm, keep_probability=0.8,\n",
    "                                                             phase_train=train_mode,\n",
    "                                                             weight_decay=1e-5)\n",
    "\n",
    "gender = tf.argmax(tf.nn.softmax(gender_logits), 1)\n",
    "age_ = tf.cast(tf.constant([i for i in range(0, 101)]), tf.float32)\n",
    "age = tf.reduce_sum(tf.multiply(tf.nn.softmax(age_logits), age_), axis=1)\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer())\n",
    "sess.run(init_op)\n",
    "saver = tf.train.Saver()\n",
    "ckpt = tf.train.get_checkpoint_state(\"../gender_age_tf/models/\")\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    print(\"restore model!\")\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point known face encoding in face center  0.394357258312\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-21b8d4f80b36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"point known face encoding in face center \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mindex_point_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mname_face\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_names_in_room\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_point_min\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delete from people_inroom where name=?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname_face\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# def main(sess,age,gender,train_mode,images_pl):\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n",
    "name = \"\"\n",
    "frame_number = 0\n",
    "total_in_room = 0\n",
    "total_known_in_room = 0\n",
    "index = 0\n",
    "faces = np.empty((1,160, 160, 3))\n",
    "\n",
    "# Load known_face_encoding_array and known_face name from npy\n",
    "known_face_encodings_array = np.load(\"../data/numpy/known_face_encoding.npy\")\n",
    "known_face_names = np.load(\"../data/numpy/known_face_names.npy\")\n",
    "\n",
    "# Convert nparray -> list \n",
    "number_person = len(known_face_encodings_array)\n",
    "known_face_encodings_array = known_face_encodings_array.reshape(number_person, 128)\n",
    "known_face_encodings = []\n",
    "for i in range(len(known_face_encodings_array)):\n",
    "    known_face_encodings.append(known_face_encodings_array[i])\n",
    "\n",
    "# face encodings of people in room\n",
    "face_encodings_in_room = []\n",
    "face_names_in_room = []\n",
    "face_encodings_unknown_in_room = []\n",
    "face_names_unknown_in_room = []\n",
    "# known_face_encodings: array known_face_encodings \n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "fa = FaceAligner(predictor, desiredFaceWidth=160)\n",
    "\n",
    "# Capture from camera of own computer\n",
    "# video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Capture from camera of extern computer\n",
    "video_capture_extern = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "#     ret, frame = video_capture.read()    \n",
    "    ret_extern, frame_extern = video_capture_extern.read()\n",
    "    \n",
    "    frame_number += 1\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "#     rgb_frame = frame[:, :, ::-1]\n",
    "    rgb_frame_extern = frame_extern[:, :, ::-1]\n",
    "    \n",
    "    # Use computer configuration low\n",
    "#     if (frame_number % 3 == 0):\n",
    "\n",
    "    # Find all the faces and face encodings in the current frame of video\n",
    "    face_locations = face_recognition.face_locations(rgb_frame_extern)\n",
    "#         print(face_locations)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame_extern, face_locations)\n",
    "\n",
    "    face_names = []\n",
    "    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "        face_unknown = frame_extern[top:bottom, left:right]\n",
    "#         face_unknown = cv2.cvtColor(face_unknown, cv2.COLOR_BGR2RGB)\n",
    "#         face_unknown_gray = cv2.cvtColor(face_unknown, cv2.COLOR_BGR2GRAY)\n",
    "        frame_gray = cv2.cvtColor(rgb_frame_extern, cv2.COLOR_BGR2GRAY)\n",
    "        # test face center to process\n",
    "        if face_center(face_unknown) == True:\n",
    "            # See if the face is a match for the known face(s)\n",
    "            matches = face_recognition.compare_faces(face_encodings_in_room, face_encoding, tolerance=0.4)\n",
    "\n",
    "#             print(face_recognition.face_distance(known_face_encodings, face_encoding))\n",
    "#             print(matches)\n",
    "#             print(np.min(face_recognition.face_distance(known_face_encodings, face_encoding)))\n",
    "\n",
    "            distance = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "            point = np.min(distance)\n",
    "            print(\"point known face encoding in face center \", point)\n",
    "            index_point_min = np.argmin(distance)\n",
    "            name_face = face_names_in_room[index_point_min]\n",
    "            \n",
    "            c.execute(\"delete from people_inroom where name=?\", (name_face,))\n",
    "            people.commit()\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "people.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
