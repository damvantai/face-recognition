{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "from PIL import Image\n",
    "import glob\n",
    "import re\n",
    "import ntpath\n",
    "import os\n",
    "import dlib\n",
    "import time\n",
    "from imutils.face_utils import FaceAligner\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list_file = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in glob.glob('/home/damvantai/Documents/face_recognition_demo/data/pictures_of_people_i_know/*'):\n",
    "    image = Image.open(filename)\n",
    "    image_list_file.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(image_list_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_image = ntpath.split(ntpath.split(image_list_file[1])[0])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntpath.split(image_list_file[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = []\n",
    "image_names = [ntpath.split(name)[1].split('.')[0] for name in image_list_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_face_encodings = []\n",
    "known_face_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for i, filename in enumerate(image_list_file):\n",
    "    image = face_recognition.load_image_file(filename)\n",
    "    try:\n",
    "        temp_encoding = face_recognition.face_encodings(image)[0]\n",
    "        known_face_encodings.append(temp_encoding)\n",
    "        known_face_names.append(image_names[i])\n",
    "    except IndexError:\n",
    "        continue\n",
    "end = time.time()\n",
    "print(\"Time for read file and encoding: \", (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(known_face_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(known_face_encodings))\n",
    "print(type(known_face_encodings[0]))\n",
    "print(type(known_face_encodings[0][0]))\n",
    "print(known_face_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(type(known_face_encodings))\n",
    "print(type(known_face_encodings[0]))\n",
    "# print(known_face_encodings.shape)\n",
    "print(known_face_encodings[0].shape)\n",
    "print(type(known_face_encodings[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_face_encodings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_face_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_face_encodings = np.load(\"../data/numpy/known_face_encoding.npy\")\n",
    "known_face_names = np.load(\"../data/numpy/known_face_names.npy\")\n",
    "\n",
    "# known_face_encodings = known_face_encodings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(type(known_face_encodings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(known_face_encodings[0]))\n",
    "print(type(known_face_encodings[0][0]))\n",
    "print(type(known_face_encodings[0][0][0]))\n",
    "print(known_face_encodings[0].reshape(128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "known_face_encodings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_face_encodings = known_face_encodings.reshape(42, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(known_face_encodings.shape)\n",
    "print(known_face_encodings[0].shape)\n",
    "print(known_face_encodings[0][0])\n",
    "print(type(known_face_encodings))\n",
    "print(type(known_face_encodings[0]))\n",
    "print(type(known_face_encodings[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(known_face_encodings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_face_encodings_list = []\n",
    "for i in range(len(known_face_encodings)):\n",
    "    known_face_encodings_list.append(known_face_encodings[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_face_encodings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(known_face_encodings))\n",
    "# known_face_encodings = known_face_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "known_face_encodings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = known_face_encodings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n",
    "name = \"\"\n",
    "frame_number = 0\n",
    "known_face_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "from PIL import Image\n",
    "import glob\n",
    "import re\n",
    "import ntpath\n",
    "import os\n",
    "import dlib\n",
    "import time\n",
    "from imutils.face_utils import FaceAligner\n",
    "from imutils.face_utils import FaceAligner\n",
    "from imutils.face_utils import rect_to_bb\n",
    "import dlib\n",
    "import imutils\n",
    "from imutils import face_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_center(image):\n",
    "    image_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    rects = detector(image_gray, 1)\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        shape = predictor(image_gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        center_face = shape[37][1] - shape[46][1]\n",
    "        if center_face < 10 & center_face > -10:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.377320291275\n",
      "[False, True, False, False]\n",
      "Dam Van Tai\n",
      "0.519653968268\n",
      "unknown_0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e9d0d9cd1823>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_unknow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m#                 face_location_unknow = face_recognition.face_locations(face_unknow)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 \u001b[0mface_encoding_unknow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_encodings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_unknow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m                 \u001b[0munknown_face_encodings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_encoding_unknow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munknown_face_encodings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/face_recognition/api.py\u001b[0m in \u001b[0;36mface_encodings\u001b[0;34m(face_image, known_face_locations, num_jitters)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mface\u001b[0m \u001b[0mencodings\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mface\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \"\"\"\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mraw_landmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_raw_face_landmarks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknown_face_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"small\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_face_descriptor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_landmark_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_jitters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mraw_landmark_set\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_landmarks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/face_recognition/api.py\u001b[0m in \u001b[0;36m_raw_face_landmarks\u001b[0;34m(face_image, face_locations, model)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mface_locations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_raw_face_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mface_locations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_css_to_rect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_location\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mface_location\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mface_locations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mpose_predictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpose_predictor_68_point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/face_recognition/api.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mface_locations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_raw_face_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mface_locations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_css_to_rect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_location\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mface_location\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mface_locations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mpose_predictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpose_predictor_68_point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/face_recognition/api.py\u001b[0m in \u001b[0;36m_css_to_rect\u001b[0;34m(css)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mrect\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \"\"\"\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n",
    "name = \"\"\n",
    "frame_number = 0\n",
    "\n",
    "known_face_encodings_array = np.load(\"../data/numpy/known_face_encoding.npy\")\n",
    "known_face_names = np.load(\"../data/numpy/known_face_names.npy\")\n",
    "\n",
    "len_known_face_encoding = known_face_encodings_array.shape[0]\n",
    "known_face_encodings_array = known_face_encodings_array.reshape(len_known_face_encoding, 128)\n",
    "\n",
    "known_face_encodings = []\n",
    "for i in range(len(known_face_encodings_array)):\n",
    "    known_face_encodings.append(known_face_encodings_array[i])\n",
    "\n",
    "unknown_face_encodings = []\n",
    "    \n",
    "# Capture from camera of own computer\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "    \n",
    "    # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "    # Frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "    frame_number += 1\n",
    "    \n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    rgb_small_frame = frame[:, :, ::-1]\n",
    "        \n",
    "    # Use computer configuration low\n",
    "    if (frame_number % 10 == 0):\n",
    "        # Only process every other frame of video to save time\n",
    "        if process_this_frame:\n",
    "            # Find all the faces and face encodings in the current frame of video\n",
    "            face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "            # print(face_locations)\n",
    "            face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "            face_names = []\n",
    "            for face_encoding in face_encodings:\n",
    "\n",
    "                # See if the face is a match for the known face(s)\n",
    "                matches = face_recognition.compare_faces(known_face_encodings, face_encoding, tolerance=0.4)\n",
    "\n",
    "#                 print(face_recognition.face_distance(known_face_encodings, face_encoding))\n",
    "                point = np.min(face_recognition.face_distance(known_face_encodings, face_encoding))\n",
    "                print(point)\n",
    "                \n",
    "                # Create name for unknow people\n",
    "                if point > 0.4:\n",
    "                    if os.path.isfile(\"../data/unknow_people/unknown_0.jpg\"):\n",
    "                        a = os.listdir(\"../data/unknow_people/\")\n",
    "                        a.sort()\n",
    "                        index = int(a[-1].split('.')[0].split('_')[1])\n",
    "                        index += 1\n",
    "                        path = \"../data/unknow_people/unknown_\" + str(index) + \".jpg\"\n",
    "    #                     cv2.imwrite(path, )\n",
    "                        name = \"unknown_\" + str(index)\n",
    "                    else:\n",
    "                        name = \"unknown_0\"\n",
    "                \n",
    "                \n",
    "                \n",
    "                # If a match was found in known_face_encodings, just use the first one.\n",
    "                if True in matches:\n",
    "                    print(matches)\n",
    "                    first_match_index = matches.index(True)\n",
    "                    name = known_face_names[first_match_index]\n",
    "                face_names.append(name)\n",
    "                print(name)\n",
    "    # process_this_frame = not process_this_frame\n",
    "\n",
    "\n",
    "    # Display the results\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        first_name = name.split(\"_\")[0]\n",
    "        if first_name == \"unknown\":\n",
    "            face_unknow = frame[top:bottom, left:right]\n",
    "            if face_center(face_unknow) == True:\n",
    "                path = \"../data/unknow_people/\" + name + \".jpg\"\n",
    "                cv2.imwrite(path, face_unknow)\n",
    "#                 face_location_unknow = face_recognition.face_locations(face_unknow)\n",
    "                face_encoding_unknow = face_recognition.face_encodings(face_unknow, (top, right, bottom, left))\n",
    "                unknown_face_encodings.append(face_encoding_unknow)\n",
    "                print(len(unknown_face_encodings))\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "        \n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ntpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = cv2.imread(\"../data/unknow_people/unknown_0.jpg\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('image', abc)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/unknow_people/unknown_' + str(4) + \".jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(path, abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"unknown_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = os.listdir(\"../data/unknow_people/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unknown_0.jpg', 'unknown_1.jpg', 'unknown_2.jpg', 'unknown_4.jpg']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown_1.jpg\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(\"../data/unknow_people/unknown_0.jpg\"):\n",
    "    a = os.listdir(\"../data/unknow_people/\")\n",
    "    print(a[-1])\n",
    "else:\n",
    "    print(\"sai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unknown'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(a[-1].split('.')[0].split('_')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# known_face_encodings = np.load(\"../data/numpy/known_face_encoding.npy\")\n",
    "# known_face_names = np.load(\"../data/numpy/known_face_names.npy\")\n",
    "\n",
    "# # for face detection\n",
    "# detector = dlib.get_frontal_face_detector()\n",
    "# predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "# fa_image = FaceAligner(predictor, desiredFaceWidth=160)\n",
    "\n",
    "# # Capture from camera of own computer\n",
    "# video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# while True:\n",
    "#     # Grab a single frame of video\n",
    "#     ret, frame = video_capture.read()\n",
    "\n",
    "#     # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "#     # Frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "#     frame_number += 1\n",
    "# #     \n",
    "#     # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "#     rgb_small_frame = frame[:, :, ::-1]\n",
    "        \n",
    "#     # Use computer configuration low\n",
    "#     if (frame_number % 10 == 0):\n",
    "#         face_encodings = []\n",
    "#         # Only process every other frame of video to save time\n",
    "#         if process_this_frame:\n",
    "#             # Find all the faces and face encodings in the current frame of video\n",
    "# #             face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "# #             print(face_locations)\n",
    "# #             face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "            \n",
    "#             # Align face\n",
    "#             image_gray = cv2.cvtColor(rgb_small_frame, cv2.COLOR_RGB2GRAY)\n",
    "#             detected_gray = detector(image_gray, 2)\n",
    "#             print(detected_gray)\n",
    "#             for i, d in enumerate(detected_gray):\n",
    "#                 image_align = fa_image.align(rgb_small_frame, image_gray, detected_gray[i])\n",
    "#                 face_encodings.append(face_recognition.face_encodings(image_align))\n",
    "            \n",
    "            \n",
    "#             face_names = []\n",
    "#             face_encodings = np.asarray(face_encodings)\n",
    "#             for face_encoding in face_encodings:\n",
    "#                 if len(face_encoding) > 0:\n",
    "#                     face_encoding = np.asarray(face_encodings).reshape(128, )\n",
    "\n",
    "#                     # See if the face is a match for the known face(s)\n",
    "#                     matches = face_recognition.compare_faces(known_face_encodings, face_encoding, tolerance=0.4)\n",
    "\n",
    "#         #             print(face_recognition.face_distance(known_face_encodings, face_encoding))\n",
    "#                     print(np.min(face_recognition.face_distance(known_face_encodings, face_encoding)))\n",
    "#                     name = \"Unknown\"\n",
    "\n",
    "#                     # If a match was found in known_face_encodings, just use the first one.\n",
    "#                     if True in matches:\n",
    "#                         first_match_index = matches.index(True)\n",
    "#                         name = known_face_names[first_match_index]\n",
    "\n",
    "#                     print(name)\n",
    "\n",
    "#                     face_names.append(name)\n",
    "#         for d, name in zip(detected_gray, face_names):\n",
    "#             # Draw a label with a name below the face\n",
    "#             cv2.rectangle(frame, (d.left(), d.bottom() - 35), (d.right(), d.bottom()), (0, 0, 255), cv2.FILLED)\n",
    "#             font = cv2.FONT_HERSHEY_DUPLEX\n",
    "#             cv2.putText(frame, name, (d.left() + 6, d.bottom() - 6), font, 1.0, (255, 255, 255), 1)\n",
    "            \n",
    "    \n",
    "#     # Display the results\n",
    "#     for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "#         # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "# #         top *= 4\n",
    "# #         right *= 4\n",
    "# #         bottom *= 4\n",
    "# #         left *= 4\n",
    "\n",
    "#         # Draw a box around the face\n",
    "#         cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "        \n",
    "#         # Draw a label with a name below the face\n",
    "#         cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "#         font = cv2.FONT_HERSHEY_DUPLEX\n",
    "#         cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "#     # Display the resulting image\n",
    "#     cv2.imshow('Video', frame)\n",
    "\n",
    "#     # Hit 'q' on the keyboard to quit!\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
